{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************************************************************\n",
      "                                LNN Model\n",
      "\n",
      "OPEN Exists: (âˆƒ0, foursides(0))                             TRUE (1.0, 1.0)\n",
      "\n",
      "AXIOM Forall: (âˆ€0, (rectangle(0) â†’ foursides(0)))           TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Implies: (rectangle(0) â†’ foursides(0)) \n",
      "('c',)                                                      TRUE (1.0, 1.0)\n",
      "('k',)                                                      TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Predicate: foursides \n",
      "('c',)                                                      TRUE (1.0, 1.0)\n",
      "('k',)                                                      TRUE (1.0, 1.0)\n",
      "\n",
      "AXIOM Forall: (âˆ€0, (square(0) â†’ rectangle(0)))              TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Implies: (square(0) â†’ rectangle(0)) \n",
      "('c',)                                                      TRUE (1.0, 1.0)\n",
      "('k',)                                                      TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Predicate: rectangle \n",
      "('c',)                                                      TRUE (1.0, 1.0)\n",
      "('k',)                                                      TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Predicate: square \n",
      "('c',)                                                      TRUE (1.0, 1.0)\n",
      "('k',)                                                      TRUE (1.0, 1.0)\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from lnn import (Predicate, Variable,\n",
    "                 Exists, Implies, Forall, Model, Fact, World)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Variablle\n",
    "x = Variable('x')\n",
    "\n",
    "# Predicate declarations\n",
    "# grounding predicates\n",
    "square = Predicate('square')\n",
    "# inferred predicates\n",
    "rectangle = Predicate('rectangle')\n",
    "foursides = Predicate('foursides')\n",
    "\n",
    "# Axioms declarations\n",
    "square_rect = Forall(x, Implies(square(x), rectangle(x)))\n",
    "rect_foursides = Forall(x, Implies(rectangle(x), foursides(x)))\n",
    "\n",
    "# Add predicates and rules to the model\n",
    "model.add_knowledge(square_rect, rect_foursides, world=World.AXIOM)\n",
    "\n",
    "model.set_query(Exists(x, foursides(x)))\n",
    "\n",
    "# Add facts to the model\n",
    "model.add_data({square: {'c': Fact.TRUE, 'k': Fact.TRUE}})\n",
    "\n",
    "# Perform inference\n",
    "steps, facts_inferred = model.infer()\n",
    "\n",
    "GT_o = dict([(\"c\", Fact.TRUE), (\"k\", Fact.TRUE)])\n",
    "# model.print()\n",
    "assert all([model.query.state(groundings=g) is GT_o[g] for g in GT_o]), \"FAILED ðŸ˜”\"\n",
    "\n",
    "model.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raining_implies_ground_is_wet = True\n",
    "raining = True\n",
    "ground_is_wet = None\n",
    "\n",
    "if raining_implies_ground_is_wet and raining:\n",
    "    ground_is_wet = True\n",
    "\n",
    "ground_is_wet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ground is wet'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'is raining'\n",
    "b = 'ground is wet'\n",
    "\n",
    "a_b = lambda a:b\n",
    "\n",
    "a_b(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORALL(implies_mortal) over the domain:\n",
      "TRUE\n",
      "implies_mortal(x) with x='human':\n",
      "TRUE\n",
      "implies_mortal(x) with x='alien':\n",
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "# Lambda calculus definitions\n",
    "TRUE = lambda t: lambda f: t\n",
    "FALSE = lambda t: lambda f: f\n",
    "\n",
    "# Helper functions\n",
    "to_boolean = lambda p: p(True)(False)\n",
    "\n",
    "def print_boolean(lc_boolean):\n",
    "    print(\"TRUE\" if to_boolean(lc_boolean) else \"FALSE\")\n",
    "\n",
    "NOT = lambda m: m(FALSE)(TRUE)\n",
    "AND = lambda m: lambda n: m(n)(FALSE)\n",
    "OR = lambda m: lambda n: m(TRUE)(n)\n",
    "\n",
    "# Correct IMPLIES definition using Â¬P âˆ¨ Q\n",
    "IMPLIES = lambda p: lambda q: OR(NOT(p))(q)\n",
    "\n",
    "# Redefine predicates to return lambda calculus booleans\n",
    "isHuman = lambda x: TRUE if x == \"human\" else FALSE\n",
    "isMortal = lambda x: TRUE if x == \"human\" else FALSE  # Simplified for demonstration\n",
    "\n",
    "# Define a foldl function to apply AND over the list\n",
    "def foldl(f, acc, xs):\n",
    "    for x in xs:\n",
    "        acc = f(acc, x)\n",
    "    return acc\n",
    "\n",
    "# Define FORALL as a lambda function to check a predicate over all values in a domain\n",
    "FORALL = lambda P: lambda domain: \\\n",
    "    foldl(lambda acc, x: AND(acc)(P(x)), TRUE, domain)\n",
    "\n",
    "# Define the predicate for implication\n",
    "implies_mortal = lambda x: IMPLIES(isHuman(x))(isMortal(x))\n",
    "\n",
    "# Define the domain of values\n",
    "domain = [\"human\", \"alien\"]\n",
    "\n",
    "# Apply FORALL to the predicate over the domain\n",
    "result = FORALL(implies_mortal)(domain)\n",
    "print(\"FORALL(implies_mortal) over the domain:\")\n",
    "print_boolean(result)  # Expected output: TRUE or FALSE based on the logic\n",
    "\n",
    "# Optionally, test individual values to understand their contributions\n",
    "for x in domain:\n",
    "    result = implies_mortal(x)\n",
    "    print(f\"implies_mortal(x) with x='{x}':\")\n",
    "    print_boolean(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93msample_grammars\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('sample_grammars')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mgrammars/sample_grammars/sem2.fcfg\u001b[0m\n\n  Searched in:\n    - '/home/tduval/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logic\n\u001b[1;32m      4\u001b[0m sents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmary walks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret_sents\u001b[49m\u001b[43m(\u001b[49m\u001b[43msents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrammars/sample_grammars/sem2.fcfg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/sem/util.py:87\u001b[0m, in \u001b[0;36minterpret_sents\u001b[0;34m(inputs, grammar, semkey, trace)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpret_sents\u001b[39m(inputs, grammar, semkey\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEM\u001b[39m\u001b[38;5;124m\"\u001b[39m, trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Add the semantic representation to each syntactic parse tree\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    of each input sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    :rtype: list(list(tuple(nltk.tree.Tree, nltk.sem.logic.ConstantExpression)))\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     86\u001b[0m         [(syn, root_semrep(syn, semkey)) \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m syntrees]\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m syntrees \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_sents\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/sem/util.py:43\u001b[0m, in \u001b[0;36mparse_sents\u001b[0;34m(inputs, grammar, trace)\u001b[0m\n\u001b[1;32m     41\u001b[0m     cp \u001b[38;5;241m=\u001b[39m FeatureChartParser(grammar)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[43mload_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m parses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/parse/util.py:56\u001b[0m, in \u001b[0;36mload_parser\u001b[0;34m(grammar_url, trace, parser, chart_class, beam_size, **load_args)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_parser\u001b[39m(\n\u001b[1;32m     23\u001b[0m     grammar_url, trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, parser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, chart_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mload_args\n\u001b[1;32m     24\u001b[0m ):\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Load a grammar from a file, and build a parser based on that grammar.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    The parser depends on the grammar format, and might also depend\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        See ``data.load`` for more information.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     grammar \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrammar_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grammar, CFG):\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe grammar must be a CFG, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a subclass thereof.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93msample_grammars\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('sample_grammars')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mgrammars/sample_grammars/sem2.fcfg\u001b[0m\n\n  Searched in:\n    - '/home/tduval/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
